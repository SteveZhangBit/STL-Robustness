###################### runner configs ######################
mode: 'train'
seed: 0
device: "cpu"
# if device is gpu, specify the gpu id
device_id: 0
# if device is cpu, specify the thread num
threads: 4
policy_name: "sac_lag"

# training configs
epochs: 500
save_best_epoch: 0
warmup: False
evaluate_episode_num: 5
save_every_epoch: 10
# compute the score of a model to save; score = - cost + reward/normalizer
reward_normalizer: 20
eval_attackers: ["mad", "amad", "uniform", "max_cost", "max_reward"]

exp_name: null
# data dir to save the logger files
data_dir: null
load_dir: null
verbose: True

###################### env configs ######################
env_cfg:
    env_name: 'SafetyAntRun-v0'
    # Maximum steps per episode, use this to terminate one episode if it takes too many steps.
    # This is a environment-specific parameter. Determine this carefully based on your gym env.
    # If this is -1, it will be based on the env._max_episode_steps
    timeout_steps: 300
    cost_limit: &COST_LIM 5
    # cost normalizer is used to scale the binary indicator cost. The cost in the replay buffer would be 1/cost_normalizer
    cost_normalizer: &CN 5

###################### adversary configs ######################
adv_cfg:
    noise_scale: &NS 0.05
    attack_freq: 1
    # MAD attacker config
    mad_cfg: &MAD
        lr_adv: 0.1
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
    amad_cfg:
        lr_adv: 0.1
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
        # only attack the states with attack_fraction highest qc values
        attack_fraction: 0.2

    # Critic attacker config
    max_cost_cfg:
        lr_adv: 0.2
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
        reward_weight: 0
        cost_weight: 0.5
    max_reward_cfg:
        lr_adv: 0.2
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
        reward_weight: 0.5
        cost_weight: 0
    min_reward_cfg:
        lr_adv: 0.1
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
        reward_weight: -0.5
        cost_weight: 0

    uniform_cfg: {}
    gaussian_cfg: {}

policy_cfg:
    safe_rl: True
    actor_lr: 0.001
    critic_lr: 0.001
    # actor critic model_config:
    hidden_sizes: [256, 256]
    # Entropy regularization coefficient.
    gamma: 0.99
    polyak: 0.995
    num_q: 2
    batch_size: 300
    buffer_size: 80000
    sample_episode_num: 20
    episode_rerun_num: 40
    num_qc: 1
    alpha: 0.0085

    n_step: 1
    use_retrace: False
    unroll_length: 10
    td_error_lim: 10
    grad_norm_lim: 0.01
    retrace_lambda: 1

    ################ adversarial training ################
    # "vanilla_adv", "vanilla", "random"
    rs_mode: "vanilla"
    attacker: "uniform"
    adv_start_epoch: 50
    adv_incr_epoch: 100