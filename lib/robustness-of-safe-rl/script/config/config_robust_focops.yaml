###################### runner configs ######################
mode: 'train'
seed: 0
device: "cpu"
# if device is gpu, specify the gpu id
device_id: 0
# if device is cpu, specify the thread num
threads: 4
policy_name: "robust_focops"

# training configs
epochs: 500
save_best_epoch: 0
save_every_epoch: 10
warmup: False
evaluate_episode_num: 5
# compute the score of a model to save; score = - cost + reward/normalizer
reward_normalizer: 20
eval_attackers: ["mad", "amad", "uniform", "max_cost", "max_reward"]

exp_name: null
# data dir to save the logger files
data_dir: null
load_dir: null
verbose: True

suffix: null

###################### env configs ######################
env_cfg:
    env_name: 'SafetyAntRun-v0'
    # Maximum steps per episode, use this to terminate one episode if it takes too many steps.
    # This is a environment-specific parameter. Determine this carefully based on your gym env.
    # If this is -1, it will be based on the env._max_episode_steps
    timeout_steps: 300
    cost_limit: &COST_LIM 5
    # cost normalizer is used to scale the binary indicator cost. The cost in the replay buffer would be 1/cost_normalizer
    cost_normalizer: &CN 5

###################### adversary configs ######################
adv_cfg:
    noise_scale: &NS 0.05
    attack_freq: 1
    # MAD attacker config
    mad_cfg: &MAD
        lr_adv: 0.1
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
    amad_cfg: 
        lr_adv: 0.05
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
        # only attack the states with attack_fraction highest qc values
        attack_fraction: 0.1

    # Critic attacker config
    max_cost_cfg:
        lr_adv: 0.1
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
        reward_weight: 0
        cost_weight: 0.5
    max_reward_cfg:
        lr_adv: 0.1
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
        reward_weight: 0.5
        cost_weight: 0
    min_reward_cfg:
        lr_adv: 0.1
        max_iterations: 60
        tol: 1.0e-4
        eps_tol: 1.0e-4
        kl_thres: 10000000
        reward_weight: -0.5
        cost_weight: 0

    uniform_cfg: {}
    gaussian_cfg: {}

policy_cfg:
    safe_rl: True
    actor_lr: 0.0003
    critic_lr: 0.001
    hidden_sizes: [256, 256]
    gamma: 0.99
    polyak: 0.995
    # robust training mode: "mc", "mr", "mcv", "mrv"
    # baseline mode: "vanilla", "kl", "klmc", "klmr", "mad", "madv", "uniform", "gaussian"
    rs_mode: "vanilla"
    kl_coef: 10
    lam: 0.97
    batch_size: 300
    buffer_size: 100000
    # FOCOPS specific config
    train_actor_iters: 80
    update_adv_freq: 40
    train_critic_iters: 80
    eval_attack_freq: 1
    interact_steps: &IS 30000
    episode_rerun_num: 40
    nu: 0
    nu_lr: 0.01
    nu_max: 2.0
    l2_reg: 0.001
    eta: 0.02
    delta: 0.02
    tem_lam: 1.5
    # placeholder
    attacker_names: ["uniform"]
    start_epoch: 20
    decay_epoch: 100